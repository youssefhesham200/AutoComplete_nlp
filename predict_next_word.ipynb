{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssefhesham200/predict_next_word_nlp/blob/master/predict_next_word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ErUZi7YlkNe",
        "outputId": "34cdb9bf-2a00-4702-a97e-cdba1e7b3b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tarfile\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gensim\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktaOvaSyU0fW"
      },
      "outputs": [],
      "source": [
        "# Download a pretrained word2vec model\n",
        "word2vec_model =  api.load(\"word2vec-google-news-300\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI-8BagPfCK1",
        "outputId": "2af91a34-ac6c-41a9-a191-e881addef3dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('good', 0.6836091876029968),\n",
              " ('lovely', 0.6676310896873474),\n",
              " ('neat', 0.6616737246513367),\n",
              " ('fantastic', 0.6569240689277649),\n",
              " ('wonderful', 0.6561347246170044),\n",
              " ('terrific', 0.6552367806434631),\n",
              " ('great', 0.6454657912254333),\n",
              " ('awesome', 0.6404187679290771),\n",
              " ('nicer', 0.6302445530891418),\n",
              " ('decent', 0.5993332862854004)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "word2vec_model.most_similar(\"nice\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(word_index, embedding_dim):\n",
        "    # Create the embedding matrix using the pre-trained Word2Vec model\n",
        "    num_words = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if word in word2vec_model.key_to_index:\n",
        "            embedding_matrix[i] = word2vec_model.get_vector(word)\n",
        "        else:\n",
        "            # If word is out-of-vocabulary, use the average of constituent word embeddings\n",
        "            words = word.split()\n",
        "            if all(w in word2vec_model.key_to_index for w in words):\n",
        "                embedding_matrix[i] = np.mean([word2vec_model.get_vector(w) for w in words], axis=0)\n",
        "            else:\n",
        "                # If word is still out-of-vocabulary, use standardized token\n",
        "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "p-xrhnlUyCsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMJqlzZTCVSV"
      },
      "outputs": [],
      "source": [
        "def extract_data():\n",
        "  tar = tarfile.open(\"enronsentv1 (1).tar.gz\", \"r:gz\")\n",
        "  tar.extractall()\n",
        "  tar.close()\n",
        "\n",
        "  \n",
        "def loading_data(folder_path):\n",
        "  # Initialize list to hold all paragraphs in the Enron Sent Corpus Dataset\n",
        "  training = []\n",
        "  validation = []\n",
        "\n",
        "  counter = 0 \n",
        "\n",
        "  # Loop through all files in the folder\n",
        "  for filename in os.listdir(folder_path):\n",
        "          # Open the file and read its contents\n",
        "          with open(os.path.join(folder_path, filename), \"r\", encoding='latin-1') as f:\n",
        "              contents = f.read()\n",
        "              para = contents.split('\\n\\n')\n",
        "              print(len(para))\n",
        "              # Split the contents into paragraphs\n",
        "          if counter <= 10:\n",
        "            # Append the paragraphs to the all_paragraphs list\n",
        "            training.extend(para)\n",
        "\n",
        "          elif counter > 10 and counter <= 15:\n",
        "            validation.extend(para)\n",
        "\n",
        "          else:\n",
        "            break\n",
        "\n",
        "          counter += 1\n",
        "\n",
        "  return training, validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnqCcNcACVZ2"
      },
      "outputs": [],
      "source": [
        "def clean(paragraphs):\n",
        "    \"\"\"Remove stopwords and lowercase words in paragraphs\"\"\"\n",
        "    cleaned_paragraphs = []\n",
        "    english_words = set(nltk.corpus.words.words())\n",
        "    for i in range(len(paragraphs)):\n",
        "        words = paragraphs[i].split()\n",
        "        non_stop = []\n",
        "        for j in range(len(words)):\n",
        "            if words[j] not in stopwords.words('english') and words[j].lower() in english_words:\n",
        "                non_stop.append(words[j].lower())\n",
        "        cleaned_paragraphs.append(' '.join(non_stop))\n",
        "    return cleaned_paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(sentences_train, sentences_val, seq_len):\n",
        "    # Tokenize the sentences\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences_train)\n",
        "    train_sequences = tokenizer.texts_to_sequences(sentences_train)\n",
        "    val_sequences = tokenizer.texts_to_sequences(sentences_val)\n",
        "    \n",
        "    # Generate input/output sequences for training data\n",
        "    train_inputs = []\n",
        "    train_outputs = []\n",
        "    for seq in train_sequences:\n",
        "        for i in range(seq_len, len(seq)):\n",
        "            train_inputs.append(seq[i-seq_len:i])\n",
        "            train_outputs.append(seq[i])\n",
        "\n",
        "    # Generate input/output sequences for validation data\n",
        "    val_inputs = []\n",
        "    val_outputs = []\n",
        "    for seq in val_sequences:\n",
        "        for i in range(seq_len, len(seq)):\n",
        "            val_inputs.append(seq[i-seq_len:i])\n",
        "            val_outputs.append(seq[i])\n",
        "\n",
        "    # Reshape the input sequences to have the correct shape\n",
        "    train_inputs = np.asarray(train_inputs).reshape(-1, seq_len, 1)\n",
        "    val_inputs = np.asarray(val_inputs).reshape(-1, seq_len, 1)\n",
        "\n",
        "    return train_inputs, train_outputs, val_inputs, val_outputs , tokenizer.word_index, tokenizer"
      ],
      "metadata": {
        "id": "4n3ORXlHfR5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-brOu1sRAax"
      },
      "outputs": [],
      "source": [
        "extract_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG-XwbjfPABa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2adccae-3d3e-46be-eb93-78bc887bfe1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13797\n",
            "12521\n",
            "8173\n",
            "11867\n",
            "9143\n",
            "7534\n",
            "12781\n",
            "12755\n",
            "9625\n",
            "8980\n",
            "13261\n",
            "12562\n",
            "11314\n",
            "11370\n",
            "11565\n",
            "13337\n",
            "17\n"
          ]
        }
      ],
      "source": [
        "paragraphs_training, paragraphs_validation = loading_data(\"/content/enronsent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IhEuTPjB9_H"
      },
      "outputs": [],
      "source": [
        "clean_training = clean(paragraphs_training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_valid = clean(paragraphs_validation)"
      ],
      "metadata": {
        "id": "EbjvSTNNZ6ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, train_outputs, val_inputs, val_outputs, word_index, tokenizer = prepare_data(clean_training, clean_valid, 10)"
      ],
      "metadata": {
        "id": "uS_0usxCelou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = load_embeddings(word_index, 300)"
      ],
      "metadata": {
        "id": "kOVZ38leK9qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#note :::  this results of accuracy is normal because data is small for (auto complete task) compare with other datasets so this lead to overfiting after all trying to solve it \n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Build the LSTM model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
        "                              weights=[embedding_matrix], input_length=10),\n",
        "\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "\n",
        "    tf.keras.layers.Dense(embedding_matrix.shape[0], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer= Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_inputs, np.array(train_outputs), epochs=35, batch_size = 256,  validation_data=(val_inputs,  np.array(val_outputs)))"
      ],
      "metadata": {
        "id": "44kZSp0zLx9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e05baf1-c30b-45ca-dd5f-91e317eef9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "2264/2264 [==============================] - 77s 32ms/step - loss: 7.1278 - accuracy: 0.0479 - val_loss: 6.7764 - val_accuracy: 0.0777\n",
            "Epoch 2/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 6.3386 - accuracy: 0.0884 - val_loss: 6.6566 - val_accuracy: 0.0939\n",
            "Epoch 3/35\n",
            "2264/2264 [==============================] - 41s 18ms/step - loss: 5.7316 - accuracy: 0.1314 - val_loss: 6.7039 - val_accuracy: 0.1009\n",
            "Epoch 4/35\n",
            "2264/2264 [==============================] - 42s 19ms/step - loss: 5.1911 - accuracy: 0.1810 - val_loss: 6.8231 - val_accuracy: 0.1020\n",
            "Epoch 5/35\n",
            "2264/2264 [==============================] - 42s 19ms/step - loss: 4.7260 - accuracy: 0.2326 - val_loss: 6.9801 - val_accuracy: 0.1012\n",
            "Epoch 6/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 4.3247 - accuracy: 0.2836 - val_loss: 7.1542 - val_accuracy: 0.1036\n",
            "Epoch 7/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 3.9783 - accuracy: 0.3295 - val_loss: 7.3250 - val_accuracy: 0.1034\n",
            "Epoch 8/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 3.6790 - accuracy: 0.3709 - val_loss: 7.5218 - val_accuracy: 0.1037\n",
            "Epoch 9/35\n",
            "2264/2264 [==============================] - 35s 16ms/step - loss: 3.4174 - accuracy: 0.4092 - val_loss: 7.7267 - val_accuracy: 0.1037\n",
            "Epoch 10/35\n",
            "2264/2264 [==============================] - 41s 18ms/step - loss: 3.1867 - accuracy: 0.4423 - val_loss: 7.9243 - val_accuracy: 0.1041\n",
            "Epoch 11/35\n",
            "2264/2264 [==============================] - 40s 18ms/step - loss: 2.9829 - accuracy: 0.4727 - val_loss: 8.1051 - val_accuracy: 0.1047\n",
            "Epoch 12/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 2.8008 - accuracy: 0.5001 - val_loss: 8.3032 - val_accuracy: 0.1048\n",
            "Epoch 13/35\n",
            "2264/2264 [==============================] - 35s 15ms/step - loss: 2.6369 - accuracy: 0.5255 - val_loss: 8.5026 - val_accuracy: 0.1047\n",
            "Epoch 14/35\n",
            "2264/2264 [==============================] - 34s 15ms/step - loss: 2.4904 - accuracy: 0.5481 - val_loss: 8.6905 - val_accuracy: 0.1037\n",
            "Epoch 15/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 2.3585 - accuracy: 0.5688 - val_loss: 8.8751 - val_accuracy: 0.1051\n",
            "Epoch 16/35\n",
            "2264/2264 [==============================] - 35s 16ms/step - loss: 2.2362 - accuracy: 0.5881 - val_loss: 9.0503 - val_accuracy: 0.1056\n",
            "Epoch 17/35\n",
            "2264/2264 [==============================] - 41s 18ms/step - loss: 2.1272 - accuracy: 0.6050 - val_loss: 9.2538 - val_accuracy: 0.1069\n",
            "Epoch 18/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 2.0266 - accuracy: 0.6208 - val_loss: 9.4088 - val_accuracy: 0.1066\n",
            "Epoch 19/35\n",
            "2264/2264 [==============================] - 37s 16ms/step - loss: 1.9357 - accuracy: 0.6353 - val_loss: 9.5889 - val_accuracy: 0.1059\n",
            "Epoch 20/35\n",
            "2264/2264 [==============================] - 37s 16ms/step - loss: 1.8521 - accuracy: 0.6488 - val_loss: 9.7505 - val_accuracy: 0.1073\n",
            "Epoch 21/35\n",
            "2264/2264 [==============================] - 40s 18ms/step - loss: 1.7754 - accuracy: 0.6610 - val_loss: 9.9324 - val_accuracy: 0.1071\n",
            "Epoch 22/35\n",
            "2264/2264 [==============================] - 40s 18ms/step - loss: 1.7042 - accuracy: 0.6728 - val_loss: 10.0960 - val_accuracy: 0.1067\n",
            "Epoch 23/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 1.6390 - accuracy: 0.6832 - val_loss: 10.2546 - val_accuracy: 0.1077\n",
            "Epoch 24/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 1.5817 - accuracy: 0.6919 - val_loss: 10.4068 - val_accuracy: 0.1081\n",
            "Epoch 25/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 1.5244 - accuracy: 0.7012 - val_loss: 10.5674 - val_accuracy: 0.1082\n",
            "Epoch 26/35\n",
            "2264/2264 [==============================] - 38s 17ms/step - loss: 1.4749 - accuracy: 0.7091 - val_loss: 10.7324 - val_accuracy: 0.1086\n",
            "Epoch 27/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 1.4269 - accuracy: 0.7163 - val_loss: 10.8494 - val_accuracy: 0.1074\n",
            "Epoch 28/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 1.3809 - accuracy: 0.7244 - val_loss: 11.0184 - val_accuracy: 0.1087\n",
            "Epoch 29/35\n",
            "2264/2264 [==============================] - 36s 16ms/step - loss: 1.3424 - accuracy: 0.7301 - val_loss: 11.1683 - val_accuracy: 0.1089\n",
            "Epoch 30/35\n",
            "2264/2264 [==============================] - 38s 17ms/step - loss: 1.3056 - accuracy: 0.7356 - val_loss: 11.2866 - val_accuracy: 0.1082\n",
            "Epoch 31/35\n",
            "2264/2264 [==============================] - 35s 15ms/step - loss: 1.2690 - accuracy: 0.7419 - val_loss: 11.4154 - val_accuracy: 0.1085\n",
            "Epoch 32/35\n",
            "2264/2264 [==============================] - 34s 15ms/step - loss: 1.2368 - accuracy: 0.7461 - val_loss: 11.5682 - val_accuracy: 0.1088\n",
            "Epoch 33/35\n",
            "2264/2264 [==============================] - 39s 17ms/step - loss: 1.2097 - accuracy: 0.7505 - val_loss: 11.6680 - val_accuracy: 0.1090\n",
            "Epoch 34/35\n",
            "2264/2264 [==============================] - 35s 16ms/step - loss: 1.1734 - accuracy: 0.7569 - val_loss: 11.7968 - val_accuracy: 0.1093\n",
            "Epoch 35/35\n",
            "2264/2264 [==============================] - 40s 18ms/step - loss: 1.1499 - accuracy: 0.7602 - val_loss: 11.9306 - val_accuracy: 0.1085\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc11e55450>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_word(model, tokenizer, text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "    # Pad the sequence to the same length as the training sequences\n",
        "    sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=10)\n",
        "    prediction = model.predict(sequence)[0]\n",
        "    # Get the index of the predicted word\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    # Convert the index to the predicted word\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    return predicted_word\n"
      ],
      "metadata": {
        "id": "8mOFnES0IfnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence = \" \"\n",
        "\n",
        "while True:\n",
        "  word = input(\"Enter Next word (-1 to terminate)\")\n",
        "  if word == \"-1\":\n",
        "    break\n",
        "\n",
        "  sentence += word + \" \"\n",
        "  predicted_word = generate_next_word(model, tokenizer, sentence)\n",
        "  \n",
        "  decision = input(f\"Is your next word: “{predicted_word}”\")\n",
        "\n",
        "  if decision == \"-1\":\n",
        "    break\n",
        "\n",
        "  while decision.lower() == \"yes\":\n",
        "    sentence += predicted_word + \" \"\n",
        "    predicted_word = generate_next_word(model, tokenizer, sentence)\n",
        "    decision = input(f\"Is your next word: “{predicted_word}”\")\n",
        "  \n",
        "  \n",
        "  print(\"Sorry, \")\n",
        "\n",
        "print(\"Your final Sentence is '\" + sentence + \"'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hJWyt7Ixc_2",
        "outputId": "77446bc9-9e14-4501-fcf8-78cb64fe3572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate)financial\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Is your next word: “quick”no\n",
            "Sorry, \n",
            "Enter Next word (-1 to terminate)account\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Is your next word: “look”yes\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Is your next word: “incremental”yes\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Is your next word: “transmission”no\n",
            "Sorry, \n",
            "Enter Next word (-1 to terminate)-1\n",
            "Your final Sentence is ' financial account look incremental '\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOClsEve/kVjkHK0o4MYW/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}